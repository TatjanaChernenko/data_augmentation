{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Version of Thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def geom_distribution(i):\n",
    "        # Create a geometrical distribution for i given words:\n",
    "    p = 0.5 # parameter\n",
    "    geom = []\n",
    "    for el in range(i):\n",
    "        probability = p**(el+1)  \n",
    "        geom.append(probability)\n",
    " \n",
    "    # normalize:\n",
    "    sum_geom = 0\n",
    "    for el in geom:\n",
    "        sum_geom+=el \n",
    "    #print(\"sum geom\",sum_geom)\n",
    "    \n",
    "    for e in range(len(geom)):\n",
    "        geom[e] = geom[e]/sum_geom\n",
    "    #print(\"norm geom distr: \", geom) # normalized geometrical distribution\n",
    "    \n",
    "    # Generate a random number between [0,1]\n",
    "    if i > 1: # there are more than 1 word to replace\n",
    "        x = int.from_bytes(os.urandom(8), byteorder=\"big\") / ((1 << 64) - 1) # generate a random number \n",
    "    elif i ==1: # there is only one word to replace\n",
    "        x = 0\n",
    "    else: # if = 0 -- there are no words to replace\n",
    "        return None\n",
    "    \n",
    "    # Calculate a cumulative sum:\n",
    "    cum_sum = []\n",
    "    for el in range(len(geom)):\n",
    "        #print(el)\n",
    "        if el >0:\n",
    "            cum_sum.append(geom[el]+cum_sum[el-1])\n",
    "        else:\n",
    "            cum_sum.append(geom[el])\n",
    "\n",
    "    \n",
    "    # calculate t - number of the words to replace\n",
    "    if len(cum_sum) == 1:\n",
    "        t = 1\n",
    "        return t\n",
    "    for i in range(len(cum_sum)-1):\n",
    "        if x < cum_sum[i]:\n",
    "            t = i+1            \n",
    "            return t\n",
    "            break\n",
    "        else:\n",
    "            t = len(cum_sum)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words to replace:  3\n",
      "\n",
      "Word to repace:  have \n",
      "new word:  get \n",
      "\n",
      "\n",
      "Word to repace:  seen \n",
      "new word:  realize \n",
      "\n",
      "\n",
      "Word to repace:  quite \n",
      "new word:  rather \n",
      "\n",
      "- Input data:\n",
      " I get realize quite a few data augmentation techniques for image data. However, there's not been much work found online on data augmentation techniques for text data. Any suggestions in this regard will be appreciated.  \n",
      "\n",
      "- With Thesaurus:\n",
      " I get realize rather a few data augmentation techniques for image data. However, there's not been much work found online on data augmentation techniques for text data. Any suggestions in this regard will be appreciated.   \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "import os\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "our_input = \"I have seen quite a few data augmentation techniques for image data. However, there's not been much work found online on data augmentation techniques for text data. Any suggestions in this regard will be appreciated.\"\n",
    "#our_input = \"jfd went my man good bad expelent\"\n",
    "def thesaurus_right(our_input):\n",
    "    clean_input = [] # the words that could be replaced\n",
    "    pos_for_thesaurus = [\"JJ\",\"JJR\",\"JJS\",\"NN\",\"RB\",\"RBR\",\"RBS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "    translation = \"\"\n",
    "    punctuation = [\".\",\",\",\"?\",\"!\",\":\",\";\",\"...\",\"\\\"\",\"\\'\",\"\\'\\''\",\"(\",\")\",\"[\",\"]\",\"{\",\"}\"]\n",
    "    # Clean the words from punctuation. Choose the words that could be replaced (Nouns, Adverbs, Verbs, Adjectives)\n",
    "    for index in range(len(our_input.split(\" \"))):\n",
    "        clean_word = \"\"\n",
    "        for el in our_input.split(\" \")[index]:\n",
    "            if el not in punctuation:\n",
    "                clean_word+=el\n",
    "        try:\n",
    "            te = nltk.word_tokenize(clean_word)\n",
    "            x = nltk.pos_tag(te)\n",
    "            \n",
    "            if x[0][1] in pos_for_thesaurus:\n",
    "                x = (x,index)\n",
    "                clean_input.append(x)\n",
    "        except:\n",
    "            continue\n",
    "    # check if there are synonyms in thesaurus:  \n",
    "    new_clean_input = []\n",
    "    for w in clean_input: \n",
    "        try: \n",
    "            synon = wn.synsets(wn.morphy(w[0][0][0]))\n",
    "            if synon !=[]:\n",
    "                new_clean_input.append(w)\n",
    "\n",
    "        except:\n",
    "            synon = wn.synsets(w[0][0][0])\n",
    "            if synon !=[]:\n",
    "                new_clean_input.append(w)\n",
    "\n",
    "        else:\n",
    "            synon = None\n",
    " \n",
    "                               \n",
    "    i = len(new_clean_input)\n",
    "\n",
    "    geom_d = geom_distribution(i)\n",
    " \n",
    "    if type(geom_d)==int:\n",
    "        pass\n",
    "    else:\n",
    "        return our_input\n",
    "        \n",
    "\n",
    "    count = 0\n",
    "    num = 0\n",
    "    \n",
    "    print(\"Number of words to replace: \", geom_d)\n",
    "    for wo_to_replace in range(geom_d):\n",
    "        new_a = []\n",
    "        alle = []\n",
    "\n",
    "        while len(new_a)==0: # if we don't find the synonym in thesarius, we go to the next word to replace\n",
    "            if num < len(new_clean_input):\n",
    "                w_to_replace = new_clean_input[num][0]\n",
    "\n",
    "                try: \n",
    "                    synon = wn.synsets(wn.morphy(w_to_replace[0][0]))\n",
    "                except:\n",
    "                    synon = wn.synsets(w_to_replace[0][0])\n",
    "                #print(synon) # all meanings\n",
    "            # collect all synonyms from all meaning with the needed POS-Tag (as by word_to_translate) in the right order\n",
    "                for el in synon:\n",
    "                    a = el.lemma_names()\n",
    "                    #print(\"a: \",a) # set of synonyms with one meaning\n",
    "                    for el in a:\n",
    "                        el = nltk.word_tokenize(el)\n",
    "                        x = nltk.pos_tag(el)\n",
    "                        if w_to_replace[0][1][0:2] == x[0][1][0:2]:\n",
    "                            try:\n",
    "                                if lancaster_stemmer.stem(w_to_replace[0][0].lower())!=lancaster_stemmer.stem(x[0][0].lower()):\n",
    "                                    if x not in new_a:\n",
    "                                        new_a.append(x)\n",
    "                            except:\n",
    "                                if w_to_replace[0][0].lower()!=x[0][0].lower():  \n",
    "                                    if x not in new_a:\n",
    "                                        new_a.append(x)\n",
    "                                #print(\"DAA: \", w_to_replace[0][1][0:2],x[0][1][0:2])\n",
    "                            #print(\"DAA2: \",lancaster_stemmer.stem(w_to_replace[0][0].lower()),lancaster_stemmer.stem(x[0][0].lower()))\n",
    "                                if x not in new_a:\n",
    "                                    new_a.append(x)\n",
    "                    #print(\"new_a\",new_a)   # all collected synonyms          \n",
    "                if len(new_a)> 0:\n",
    "                    number_of_syn = geom_distribution(len(new_a)) #what synonym to chose\n",
    "                    alle.append(new_a[number_of_syn-1])\n",
    "                else: # if we don't find good synonyms:\n",
    "                    if count <len(new_clean_input): # try the next word in input                    \n",
    "                        num +=1\n",
    "                        count+=1\n",
    "                        print(num,count)\n",
    "                    else: # if there are no more good synonyms to try for the current word:\n",
    "                        new_a.append(w_to_replace) #use the same word (without replace)\n",
    "                        alle.append(new_a[number_of_syn-1])\n",
    "                        #pass\n",
    "\n",
    "            else:\n",
    "                num-=1\n",
    "                count-=1\n",
    "                new_a.append(w_to_replace)\n",
    "                alle.append(new_a[0])\n",
    "    #print(len(alle)) # always = 1           \n",
    "        if len(alle) > 0:\n",
    "            print(\"\\nWord to repace: \",w_to_replace[0][0], \"\\nnew word: \", alle[0][0][0],\"\\n\") \n",
    "            a = alle[0][0][0]\n",
    "            if a.split(\"_\"):\n",
    "                x = \"\"\n",
    "                a = a.split(\"_\")\n",
    "                for el in a:\n",
    "                    x+=el\n",
    "                    x+=\" \"\n",
    "                x.strip()\n",
    "                a = x\n",
    "   \n",
    "            s = new_clean_input[num][1]            \n",
    "            if wo_to_replace==0:\n",
    "                tra = \"\"\n",
    "                our_input = our_input\n",
    "            else:\n",
    "                our_input = tra\n",
    "                tra = \"\"\n",
    " \n",
    "            for el in range(len(our_input.split(\" \"))):\n",
    "                if el != s:\n",
    "                    tra+=our_input.split(\" \")[el]\n",
    "                    tra+=\" \"\n",
    "                else:\n",
    "                    tra+=a\n",
    "        else:\n",
    "            tra=our_input\n",
    "            print(\"No synsets for this sentence\")\n",
    "    \n",
    "        num+=1\n",
    "        count+=1\n",
    "    print(\"- Input data:\\n\", our_input)\n",
    "    print(\"\\n- With Thesaurus:\\n\", tra)\n",
    "    return tra\n",
    "if __name__ == \"__main__\":\n",
    "    thesaurus_right(our_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
