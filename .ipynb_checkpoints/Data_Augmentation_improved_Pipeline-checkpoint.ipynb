{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Augmentation with Back Translation and Thesarius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on the 4 categories of the 20Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB and SVM - with BOW, TFIDF, averaged word2vec, TFIDF weighted averaged word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the extraction functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#\n",
    "def bow_extractor(corpus, ngram_range=(1,1)):\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "    \n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    \n",
    "    transformer = TfidfTransformer(norm='l2',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "    \n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=1, \n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "    \n",
    "\n",
    "import numpy as np    \n",
    "    \n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "#   \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    #vocabulary = set(model.index2word)\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "    \n",
    "    \n",
    "def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):\n",
    "    \n",
    "    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)] \n",
    "                   if tfidf_vocabulary.get(word) \n",
    "                   else 0 for word in words]    \n",
    "    word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    wts = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            word_vector = model[word]\n",
    "            weighted_word_vector = word_tfidf_map[word] * word_vector\n",
    "            wts = wts + word_tfidf_map[word]\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "#\n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, \n",
    "                                   tfidf_vocabulary, model, num_features):\n",
    "                                       \n",
    "    docs_tfidfs = [(doc, doc_tfidf) \n",
    "                   for doc, doc_tfidf \n",
    "                   in zip(corpus, tfidf_vectors)]\n",
    "    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary,\n",
    "                                   model, num_features)\n",
    "                    for tokenized_sentence, tfidf in docs_tfidfs]\n",
    "    return np.array(features) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on the original (not augmented Dataset):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 original documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics','sci.med','talk.religion.misc']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.data = twenty_train.data[:100]\n",
    "twenty_train.target = twenty_train.target[:100]\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "bow_vectorizer, bow_train_features = bow_extractor(twenty_train.data)  \n",
    "bow_test_features = bow_vectorizer.transform(twenty_test.data) \n",
    "\n",
    "# tfidf features \n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(twenty_train.data)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_train.data]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_test.data]  \n",
    "# build word2vec model                   \n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)                  \n",
    "                   \n",
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)                                                 \n",
    "                   \n",
    "\n",
    "\n",
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model, \n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model, \n",
    "                                                                 num_features=500)\n",
    "\n",
    "\n",
    "# metrics\n",
    "\n",
    "def train_predict_evaluate_model(classifier, train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    print(np.mean(predictions == test_labels))\n",
    "    return predictions  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes with bag of words features:\n",
      "0.720295202952\n",
      "Support Vector Machine with bag of words features:\n",
      "0.583763837638\n",
      "Multinomial Naive Bayes with tfidf features :\n",
      "0.431734317343\n",
      "Support Vector Machine with tfidf features:\n",
      "0.693726937269\n",
      "Support Vector Machine with averaged word vector features:\n",
      "0.329151291513\n",
      "Support Vector Machine with tfidf weighted averaged word vector features:\n",
      "0.285608856089\n",
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)\n",
    "\n",
    "train_labels = twenty_train.target\n",
    "test_labels = twenty_test.target\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "print(\"Multinomial Naive Bayes with bag of words features:\")\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with bag of words features\n",
    "print(\"Support Vector Machine with bag of words features:\")\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "                                           \n",
    "# Multinomial Naive Bayes with tfidf features  \n",
    "print(\"Multinomial Naive Bayes with tfidf features :\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "print(\"Support Vector Machine with tfidf features:\")\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "print(\"Support Vector Machine with averaged word vector features:\")\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "print(\"Support Vector Machine with tfidf weighted averaged word vector features:\")\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "200 original documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes with bag of words features:\n",
      "0.731365313653\n",
      "Support Vector Machine with bag of words features:\n",
      "0.589667896679\n",
      "Multinomial Naive Bayes with tfidf features :\n",
      "0.577121771218\n",
      "Support Vector Machine with tfidf features:\n",
      "0.730627306273\n",
      "Support Vector Machine with averaged word vector features:\n",
      "0.372693726937\n",
      "Support Vector Machine with tfidf weighted averaged word vector features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.340221402214\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics','sci.med','talk.religion.misc']\n",
    "#categories = ['alt.atheism', 'comp.graphics']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.data = twenty_train.data[:200]\n",
    "twenty_train.target = twenty_train.target[:200]\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "bow_vectorizer, bow_train_features = bow_extractor(twenty_train.data)  \n",
    "bow_test_features = bow_vectorizer.transform(twenty_test.data) \n",
    "\n",
    "# tfidf features \n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(twenty_train.data)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_train.data]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_test.data]  \n",
    "# build word2vec model                   \n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)                  \n",
    "                   \n",
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)                                                 \n",
    "                   \n",
    "\n",
    "\n",
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model, \n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model, \n",
    "                                                                 num_features=500)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)\n",
    "\n",
    "train_labels = twenty_train.target\n",
    "test_labels = twenty_test.target\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "print(\"Multinomial Naive Bayes with bag of words features:\")\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with bag of words features\n",
    "print(\"Support Vector Machine with bag of words features:\")\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "                                           \n",
    "# Multinomial Naive Bayes with tfidf features  \n",
    "print(\"Multinomial Naive Bayes with tfidf features :\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "print(\"Support Vector Machine with tfidf features:\")\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "print(\"Support Vector Machine with averaged word vector features:\")\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "print(\"Support Vector Machine with tfidf weighted averaged word vector features:\")\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 original documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes with bag of words features:\n",
      "0.781549815498\n",
      "Support Vector Machine with bag of words features:\n",
      "0.654612546125\n",
      "Multinomial Naive Bayes with tfidf features :\n",
      "0.729889298893\n",
      "Support Vector Machine with tfidf features:\n",
      "0.783763837638\n",
      "Support Vector Machine with averaged word vector features:\n",
      "0.314391143911\n",
      "Support Vector Machine with tfidf weighted averaged word vector features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.450184501845\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics','sci.med','talk.religion.misc']\n",
    "#categories = ['alt.atheism', 'comp.graphics']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.data = twenty_train.data[:300]\n",
    "twenty_train.target = twenty_train.target[:300]\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "bow_vectorizer, bow_train_features = bow_extractor(twenty_train.data)  \n",
    "bow_test_features = bow_vectorizer.transform(twenty_test.data) \n",
    "\n",
    "# tfidf features \n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(twenty_train.data)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_train.data]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_test.data]  \n",
    "# build word2vec model                   \n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)                  \n",
    "                   \n",
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)                                                 \n",
    "                   \n",
    "\n",
    "\n",
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model, \n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model, \n",
    "                                                                 num_features=500)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)\n",
    "\n",
    "train_labels = twenty_train.target\n",
    "test_labels = twenty_test.target\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "print(\"Multinomial Naive Bayes with bag of words features:\")\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with bag of words features\n",
    "print(\"Support Vector Machine with bag of words features:\")\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "                                           \n",
    "# Multinomial Naive Bayes with tfidf features  \n",
    "print(\"Multinomial Naive Bayes with tfidf features :\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "print(\"Support Vector Machine with tfidf features:\")\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "print(\"Support Vector Machine with averaged word vector features:\")\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "print(\"Support Vector Machine with tfidf weighted averaged word vector features:\")\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2000 original documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes with bag of words features:\n",
      "0.891512915129\n",
      "Support Vector Machine with bag of words features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815498154982\n",
      "Multinomial Naive Bayes with tfidf features :\n",
      "0.828782287823\n",
      "Support Vector Machine with tfidf features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873800738007\n",
      "Support Vector Machine with averaged word vector features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.657564575646\n",
      "Support Vector Machine with tfidf weighted averaged word vector features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.661254612546\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics','sci.med','talk.religion.misc']\n",
    "#categories = ['alt.atheism', 'comp.graphics']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.data = twenty_train.data[:2000]\n",
    "twenty_train.target = twenty_train.target[:2000]\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "bow_vectorizer, bow_train_features = bow_extractor(twenty_train.data)  \n",
    "bow_test_features = bow_vectorizer.transform(twenty_test.data) \n",
    "\n",
    "# tfidf features \n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(twenty_train.data)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_train.data]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_test.data]  \n",
    "# build word2vec model                   \n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)                  \n",
    "                   \n",
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)                                                 \n",
    "                   \n",
    "\n",
    "\n",
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model, \n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model, \n",
    "                                                                 num_features=500)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)\n",
    "\n",
    "train_labels = twenty_train.target\n",
    "test_labels = twenty_test.target\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "print(\"Multinomial Naive Bayes with bag of words features:\")\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with bag of words features\n",
    "print(\"Support Vector Machine with bag of words features:\")\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "                                           \n",
    "# Multinomial Naive Bayes with tfidf features  \n",
    "print(\"Multinomial Naive Bayes with tfidf features :\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "print(\"Support Vector Machine with tfidf features:\")\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "print(\"Support Vector Machine with averaged word vector features:\")\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "print(\"Support Vector Machine with tfidf weighted averaged word vector features:\")\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on the augmented Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Back-Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 100 original documents, generate 100 new with Back translation (=200 documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes with bag of words features:\n",
      "0.735055350554\n",
      "Support Vector Machine with bag of words features:\n",
      "0.578597785978\n",
      "Multinomial Naive Bayes with tfidf features :\n",
      "0.513653136531\n",
      "Support Vector Machine with tfidf features:\n",
      "0.691512915129\n",
      "Support Vector Machine with averaged word vector features:\n",
      "0.365313653137\n",
      "Support Vector Machine with tfidf weighted averaged word vector features:\n",
      "0.332103321033\n",
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from mtranslate import translate\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics','sci.med','talk.religion.misc']\n",
    "#categories = ['alt.atheism', 'comp.graphics']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.data = twenty_train.data[:100]\n",
    "twenty_train.target = twenty_train.target[:100]\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "for i in range(len(twenty_train.data)):\n",
    "    print(i)\n",
    "    ger_transl = translate(twenty_train.data[i], \"de\")\n",
    "    new = translate(ger_transl, \"en\")\n",
    "    (twenty_train.data).append(new)\n",
    "    twenty_train.target = np.append(twenty_train.target, twenty_train.target[i])\n",
    "print(len(twenty_train.data))    \n",
    "\n",
    "bow_vectorizer, bow_train_features = bow_extractor(twenty_train.data)  \n",
    "bow_test_features = bow_vectorizer.transform(twenty_test.data) \n",
    "\n",
    "# tfidf features \n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(twenty_train.data)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_train.data]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_test.data]  \n",
    "# build word2vec model                   \n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)                  \n",
    "                   \n",
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)                                                 \n",
    "                   \n",
    "\n",
    "\n",
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model, \n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model, \n",
    "                                                                 num_features=500)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)\n",
    "\n",
    "train_labels = twenty_train.target\n",
    "test_labels = twenty_test.target\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "print(\"Multinomial Naive Bayes with bag of words features:\")\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with bag of words features\n",
    "print(\"Support Vector Machine with bag of words features:\")\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "                                           \n",
    "# Multinomial Naive Bayes with tfidf features  \n",
    "print(\"Multinomial Naive Bayes with tfidf features :\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "print(\"Support Vector Machine with tfidf features:\")\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "print(\"Support Vector Machine with averaged word vector features:\")\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "print(\"Support Vector Machine with tfidf weighted averaged word vector features:\")\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BT thes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 100 original documents, generate 200 new with Back translation with German and French as target languages (=300 documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes with bag of words features:\n",
      "0.742435424354\n",
      "Support Vector Machine with bag of words features:\n",
      "0.59557195572\n",
      "Multinomial Naive Bayes with tfidf features :\n",
      "0.561623616236\n",
      "Support Vector Machine with tfidf features:\n",
      "0.693726937269\n",
      "Support Vector Machine with averaged word vector features:\n",
      "0.371955719557\n",
      "Support Vector Machine with tfidf weighted averaged word vector features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40221402214\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from mtranslate import translate\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics','sci.med','talk.religion.misc']\n",
    "#ategories = ['alt.atheism', 'comp.graphics']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.data = twenty_train.data[:100]\n",
    "twenty_train.target = twenty_train.target[:100]\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "for i in range(len(twenty_train.data)):\n",
    "    print(i)\n",
    "    ger_transl = translate(twenty_train.data[i], \"de\")\n",
    "    new = translate(ger_transl, \"en\")\n",
    "    (twenty_train.data).append(new)\n",
    "    twenty_train.target = np.append(twenty_train.target, twenty_train.target[i])\n",
    "    fr_transl = translate(twenty_train.data[i], \"fr\")\n",
    "    new1 = translate(fr_transl, \"en\")\n",
    "    (twenty_train.data).append(new1)\n",
    "    twenty_train.target = np.append(twenty_train.target, twenty_train.target[i])\n",
    "print(len(twenty_train.data))    \n",
    "\n",
    "bow_vectorizer, bow_train_features = bow_extractor(twenty_train.data)  \n",
    "bow_test_features = bow_vectorizer.transform(twenty_test.data) \n",
    "\n",
    "# tfidf features \n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(twenty_train.data)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_train.data]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_test.data]  \n",
    "# build word2vec model                   \n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)                  \n",
    "                   \n",
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)                                                 \n",
    "                   \n",
    "\n",
    "\n",
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model, \n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model, \n",
    "                                                                 num_features=500)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)\n",
    "\n",
    "train_labels = twenty_train.target\n",
    "test_labels = twenty_test.target\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "print(\"Multinomial Naive Bayes with bag of words features:\")\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with bag of words features\n",
    "print(\"Support Vector Machine with bag of words features:\")\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "                                           \n",
    "# Multinomial Naive Bayes with tfidf features  \n",
    "print(\"Multinomial Naive Bayes with tfidf features :\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "print(\"Support Vector Machine with tfidf features:\")\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "print(\"Support Vector Machine with averaged word vector features:\")\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "print(\"Support Vector Machine with tfidf weighted averaged word vector features:\")\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Thesarius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "our_input = \"I have seen quite a few data augmentation techniques for image data. However, there's not been much work found online on data augmentation techniques for text data. Any suggestions in this regard will be appreciated.\"\n",
    "def thesarius(our_input):\n",
    "    translation = \"\"\n",
    "    punctuation = [\".\",\",\",\"?\",\"!\",\":\",\";\",\"...\",\"\\\"\",\"\\'\",\"\\'\\''\",\"(\",\")\",\"[\",\"]\",\"{\",\"}\"]\n",
    "    for word in our_input.split(\" \"):\n",
    "        clean_word = \"\"\n",
    "        for el in word:\n",
    "            if el not in punctuation:\n",
    "                clean_word+=el\n",
    "        synon = wn.synsets(clean_word)\n",
    "        alle = []\n",
    "        for el in synon:\n",
    "            a = el.lemma_names()\n",
    "            alle.append(a[0])\n",
    "        if len(alle) > 0:\n",
    "            a = alle[0]\n",
    "            if a.split(\"_\"):\n",
    "                x = \"\"\n",
    "                a = a.split(\"_\")\n",
    "                for el in a:\n",
    "                    x+=el\n",
    "                    x+=\" \"\n",
    "                x.strip()\n",
    "                a = x\n",
    "            translation+=a\n",
    "            translation+=\" \"\n",
    "        else:\n",
    "            translation+=clean_word \n",
    "            translation+=\" \"\n",
    "    if translation != our_input: \n",
    "        return translation\n",
    "    print(\"- Our data:\\n\", our_input)\n",
    "    print(\"\\n- New data:\\n\", translation)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    thesarius(our_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 100 original documents, generate 100 new with Thesarius (=200 documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multinomial Naive Bayes with bag of words features:\n",
      "0.723985239852\n",
      "Support Vector Machine with bag of words features:\n",
      "0.611070110701\n",
      "Multinomial Naive Bayes with tfidf features :\n",
      "0.532103321033\n",
      "Support Vector Machine with tfidf features:\n",
      "0.692250922509\n",
      "Support Vector Machine with averaged word vector features:\n",
      "0.287822878229\n",
      "Support Vector Machine with tfidf weighted averaged word vector features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.315867158672\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics','sci.med','talk.religion.misc']\n",
    "#categories = ['alt.atheism', 'comp.graphics']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.data = twenty_train.data[:100]\n",
    "twenty_train.target = twenty_train.target[:100]\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "for i in range(len(twenty_train.data)):\n",
    "    print(i)\n",
    "    new1 = thesarius(twenty_train.data[i])\n",
    "    (twenty_train.data).append(new1)\n",
    "    twenty_train.target = np.append(twenty_train.target, twenty_train.target[i])\n",
    "print(len(twenty_train.data))    \n",
    "\n",
    "bow_vectorizer, bow_train_features = bow_extractor(twenty_train.data)  \n",
    "bow_test_features = bow_vectorizer.transform(twenty_test.data) \n",
    "\n",
    "# tfidf features \n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(twenty_train.data)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_train.data]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_test.data]  \n",
    "# build word2vec model                   \n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)                  \n",
    "                   \n",
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)                                                 \n",
    "                   \n",
    "\n",
    "\n",
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model, \n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model, \n",
    "                                                                 num_features=500)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)\n",
    "\n",
    "train_labels = twenty_train.target\n",
    "test_labels = twenty_test.target\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "print(\"Multinomial Naive Bayes with bag of words features:\")\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with bag of words features\n",
    "print(\"Support Vector Machine with bag of words features:\")\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "                                           \n",
    "# Multinomial Naive Bayes with tfidf features  \n",
    "print(\"Multinomial Naive Bayes with tfidf features :\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "print(\"Support Vector Machine with tfidf features:\")\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "print(\"Support Vector Machine with averaged word vector features:\")\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "print(\"Support Vector Machine with tfidf weighted averaged word vector features:\")\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 1000 original documents, generate 1000 new with Thesarius (=2000 documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes with bag of words features:\n",
      "0.872324723247\n",
      "Support Vector Machine with bag of words features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787453874539\n",
      "Multinomial Naive Bayes with tfidf features :\n",
      "0.838376383764\n",
      "Support Vector Machine with tfidf features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851660516605\n",
      "Support Vector Machine with averaged word vector features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653136531365\n",
      "Support Vector Machine with tfidf weighted averaged word vector features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652398523985\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics','sci.med','talk.religion.misc']\n",
    "#categories = ['alt.atheism', 'comp.graphics']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.data = twenty_train.data[:1000]\n",
    "twenty_train.target = twenty_train.target[:1000]\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "for i in range(len(twenty_train.data)):\n",
    "    print(i)\n",
    "    new1 = thesarius(twenty_train.data[i])\n",
    "    (twenty_train.data).append(new1)\n",
    "    twenty_train.target = np.append(twenty_train.target, twenty_train.target[i])\n",
    "print(len(twenty_train.data))    \n",
    "\n",
    "bow_vectorizer, bow_train_features = bow_extractor(twenty_train.data)  \n",
    "bow_test_features = bow_vectorizer.transform(twenty_test.data) \n",
    "\n",
    "# tfidf features \n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(twenty_train.data)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_train.data]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_test.data]  \n",
    "# build word2vec model                   \n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)                  \n",
    "                   \n",
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)                                                 \n",
    "                   \n",
    "\n",
    "\n",
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model, \n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model, \n",
    "                                                                 num_features=500)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)\n",
    "\n",
    "train_labels = twenty_train.target\n",
    "test_labels = twenty_test.target\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "print(\"Multinomial Naive Bayes with bag of words features:\")\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with bag of words features\n",
    "print(\"Support Vector Machine with bag of words features:\")\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "                                           \n",
    "# Multinomial Naive Bayes with tfidf features  \n",
    "print(\"Multinomial Naive Bayes with tfidf features :\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "print(\"Support Vector Machine with tfidf features:\")\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "print(\"Support Vector Machine with averaged word vector features:\")\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "print(\"Support Vector Machine with tfidf weighted averaged word vector features:\")\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Translation with Thesarius:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 100 original documents, generate 200 new with Thesarius (=3000 documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes with bag of words features:\n",
      "0.739483394834\n",
      "Support Vector Machine with bag of words features:\n",
      "0.608118081181\n",
      "Multinomial Naive Bayes with tfidf features :\n",
      "0.557933579336\n",
      "Support Vector Machine with tfidf features:\n",
      "0.69594095941\n",
      "Support Vector Machine with averaged word vector features:\n",
      "0.323247232472\n",
      "Support Vector Machine with tfidf weighted averaged word vector features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\cher061\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.328413284133\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from mtranslate import translate\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics','sci.med','talk.religion.misc']\n",
    "#categories = ['alt.atheism', 'comp.graphics']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.data = twenty_train.data[:100]\n",
    "twenty_train.target = twenty_train.target[:100]\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "for i in range(len(twenty_train.data)):\n",
    "    print(i)\n",
    "    ger_transl = translate(twenty_train.data[i], \"de\")\n",
    "    new = translate(ger_transl, \"en\")\n",
    "    (twenty_train.data).append(new)\n",
    "    twenty_train.target = np.append(twenty_train.target, twenty_train.target[i])\n",
    "    new1 = thesarius(twenty_train.data[i])\n",
    "    (twenty_train.data).append(new1)\n",
    "    twenty_train.target = np.append(twenty_train.target, twenty_train.target[i])\n",
    "print(len(twenty_train.data))    \n",
    "\n",
    "bow_vectorizer, bow_train_features = bow_extractor(twenty_train.data)  \n",
    "bow_test_features = bow_vectorizer.transform(twenty_test.data) \n",
    "\n",
    "# tfidf features \n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(twenty_train.data)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_train.data]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in twenty_test.data]  \n",
    "# build word2vec model                   \n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)                  \n",
    "                   \n",
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)                                                 \n",
    "                   \n",
    "\n",
    "\n",
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model, \n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model, \n",
    "                                                                 num_features=500)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)\n",
    "\n",
    "train_labels = twenty_train.target\n",
    "test_labels = twenty_test.target\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "print(\"Multinomial Naive Bayes with bag of words features:\")\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with bag of words features\n",
    "print(\"Support Vector Machine with bag of words features:\")\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "                                           \n",
    "# Multinomial Naive Bayes with tfidf features  \n",
    "print(\"Multinomial Naive Bayes with tfidf features :\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "print(\"Support Vector Machine with tfidf features:\")\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "print(\"Support Vector Machine with averaged word vector features:\")\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "print(\"Support Vector Machine with tfidf weighted averaged word vector features:\")\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
